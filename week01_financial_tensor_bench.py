import torch
import time
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel

def week1_mission():
    # 1. M4 Max GPU(MPS) ê°€ì† ì„¤ì •
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f" Device: {device} (M4 Max 128GB RAM ready)")

    # 2. ê¸ˆìœµ ë°ì´í„°ì…‹ ë¡œë“œ (SEC 10-K ê¸°ë°˜)
    print("ğŸ“¥ Loading Sujet Financial RAG Dataset...")
    try:
        dataset = load_dataset("sujet-ai/Sujet-Financial-RAG-EN-Dataset", split="train")
        sample_context = dataset[0]['context'] 
        print(f" Data Sample Loaded (Length: {len(sample_context)})")
    except Exception as e:
        print(f" ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 3. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
    model_name = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)

    # 4. ì„±ëŠ¥ ì¸¡ì •: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (Embedding)
    inputs = tokenizer(sample_context[:1000], return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
    
    print("\n M4 Max MPS Embedding Speed Test...")
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    end_time = time.time()
    
    # 5. ê²°ê³¼ ì¶œë ¥
    last_hidden_state = outputs.last_hidden_state
    print(f" Tensor Shape: {last_hidden_state.shape}") # (Batch, Seq, Hidden)
    print(f" Processing Time: {(end_time - start_time)*1000:.2f} ms")
    print("\n 1ì£¼ì°¨ ë¯¸ì…˜ ì„±ê³µ! ë°ì´í„°ê°€ í…ì„œë¡œ ì™„ë²½íˆ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    week1_mission()